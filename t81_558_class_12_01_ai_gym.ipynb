{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3.7 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "t81_558_class_12_01_ai_gym.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwler123/OpenAI/blob/main/t81_558_class_12_01_ai_gym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N58_HAIBIeX6"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnID4yguIeX7"
      },
      "source": [
        "# T81-558: Applications of Deep Neural Networks\n",
        "**Module 12: Reinforcement Learning**\n",
        "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
        "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7l5rd2MIeX8"
      },
      "source": [
        "# Module 12 Video Material\n",
        "\n",
        "* **Part 12.1: Introduction to the OpenAI Gym** [[Video]](https://www.youtube.com/watch?v=_KbUxgyisjM&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_12_01_ai_gym.ipynb)\n",
        "* Part 12.2: Introduction to Q-Learning [[Video]](https://www.youtube.com/watch?v=A3sYFcJY3lA&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_12_02_qlearningreinforcement.ipynb)\n",
        "* Part 12.3: Keras Q-Learning in the OpenAI Gym [[Video]](https://www.youtube.com/watch?v=qy1SJmsRhvM&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_12_03_keras_reinforce.ipynb)\n",
        "* Part 12.4: Atari Games with Keras Neural Networks [[Video]](https://www.youtube.com/watch?v=co0SwPWoZh0&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_12_04_atari.ipynb)\n",
        "* Part 12.5: Application of Reinforcement Learning [[Video]](https://www.youtube.com/watch?v=1jQPP3RfwMI&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_12_05_apply_rl.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UImTzmGTIeX9"
      },
      "source": [
        "# Part 12.1: Introduction to the OpenAI Gym\n",
        "\n",
        "[OpenAI Gym](https://gym.openai.com/) aims to provide an easy-to-setup general-intelligence benchmark with a wide variety of different environments. The goal is to standardize how environments are defined in AI research publications so that published research becomes more easily reproducible. The project claims to provide the user with a simple interface. As of June 2017, developers can only use Gym with Python. \n",
        "\n",
        "OpenAI gym is pip-installed onto your local machine.  There are a few significant limitations to be aware of:\n",
        "\n",
        "* OpenAI Gym Atari only **directly** supports Linux and Macintosh\n",
        "* OpenAI Gym Atari can be used with Windows; however, it requires a particular [installation procedure](https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30)\n",
        "* OpenAI Gym can not directly render animated games in Google CoLab.\n",
        "\n",
        "Because OpenAI Gym requires a graphics display, the only way to display Gym in Google CoLab is an embedded video.  The presentation of OpenAI Gym game animations in Google CoLab is discussed later in this module.\n",
        "\n",
        "### OpenAI Gym Leaderboard\n",
        "\n",
        "The OpenAI Gym does have a leaderboard, similar to Kaggle; however, the OpenAI Gym's leaderboard is much more informal compared to Kaggle.  The user's local machine performs all scoring.  As a result, the OpenAI gym's leaderboard is strictly an \"honor's system.\"  The leaderboard is maintained the following GitHub repository:\n",
        "\n",
        "* [OpenAI Gym Leaderboard](https://github.com/openai/gym/wiki/Leaderboard)\n",
        "\n",
        "If you submit a score, you are required to provide a writeup with sufficient instructions to reproduce your result. A video of your results is suggested, but not required.\n",
        "\n",
        "### Looking at Gym Environments\n",
        "\n",
        "The centerpiece of Gym is the environment, which defines the \"game\" in which your reinforcement algorithm will compete.  An environment does not need to be a game; however, it describes the following game-like features:\n",
        "* **action space**: What actions can we take on the environment, at each step/episode, to alter the environment.\n",
        "* **observation space**: What is the current state of the portion of the environment that we can observe. Usually, we can see the entire environment.\n",
        "\n",
        "Before we begin to look at Gym, it is essential to understand some of the terminology used by this library.\n",
        "\n",
        "* **Agent** - The machine learning program or model that controls the actions.\n",
        "Step - One round of issuing actions that affect the observation space.\n",
        "* **Episode** - A collection of steps that terminates when the agent fails to meet the environment's objective, or the episode reaches the maximum number of allowed steps.\n",
        "* **Render** - Gym can render one frame for display after each episode.\n",
        "* **Reward** - A positive reinforcement that can occur at the end of each episode, after the agent acts.\n",
        "* **Nondeterministic** - For some environments, randomness is a factor in deciding what effects actions have on reward and changes to the observation space.\n",
        "\n",
        "It is important to note that many of the gym environments specify that they are not nondeterministic even though they make use of random numbers to process actions. It is generally agreed upon (based on the gym GitHub issue tracker) that nondeterministic property means that a deterministic environment will still behave randomly even when given consistent seed value. The seed method of an environment can be used by the program to seed the random number generator for the environment.\n",
        "\n",
        "The Gym library allows us to query some of these attributes from environments.  I created the following function to query gym environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cciTuR2MIeX-"
      },
      "source": [
        "import gym\n",
        "\n",
        "def query_environment(name):\n",
        "  env = gym.make(name)\n",
        "  spec = gym.spec(name)\n",
        "  print(f\"Action Space: {env.action_space}\")\n",
        "  print(f\"Observation Space: {env.observation_space}\")\n",
        "  print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "  print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "  print(f\"Reward Range: {env.reward_range}\")\n",
        "  print(f\"Reward Threshold: {spec.reward_threshold}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRXfAdwFDwUm"
      },
      "source": [
        "We will begin by looking at the MountainCar-v0 environment, which challenges an underpowered car to escape the valley between two mountains.  The following code describes the Mountian Car environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYwy9cjlJjEH",
        "outputId": "3b0cdd22-832a-45ce-acdd-68907cb40a4f"
      },
      "source": [
        "query_environment(\"MountainCar-v0\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(3)\n",
            "Observation Space: Box(-1.2000000476837158, 0.6000000238418579, (2,), float32)\n",
            "Max Episode Steps: 200\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: -110.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TsQiaGJE3UA"
      },
      "source": [
        "There are three distinct actions that can be taken: accelrate forward, decelerate, or accelerate backwards.  The observation space contains two continuous (floating point) values, as evident by the box object. The observation space is simply the position and velocity of the car.  The car has 200 steps to escape for each epasode.  You would have to look at the code to know, but the mountian car recieves no incramental reward.  The only reward for the car is given when it escapes the valley.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF4n5cYEMyru",
        "outputId": "1d8672b4-ad18-408c-8b0f-cc5eaa9df836"
      },
      "source": [
        "query_environment(\"CartPole-v1\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(2)\n",
            "Observation Space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "Max Episode Steps: 500\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: 475.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwvVKrNebUHJ"
      },
      "source": [
        "The CartPole-v1 environment challenges the agent to move a cart while keeping a pole balanced. The environment has an observation space of 4 continuous numbers:\n",
        "\n",
        "* Cart Position\n",
        "* Cart Velocity\n",
        "* Pole Angle\n",
        "* Pole Velocity At Tip\n",
        "\n",
        "To achieve this goal, the agent can take the following actions:\n",
        "\n",
        "* Push cart to the left\n",
        "* Push cart to the right\n",
        "\n",
        "There is also a continuous variant of the mountain car.  This version does not simply have the motor on or off.  For the continuous car the action space is a single floating point number that specifies how much forward or backward force is being applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "UAlaMcJmNSY0",
        "outputId": "456d2977-9939-40c1-ad13-4576eeff0b38"
      },
      "source": [
        "query_environment(\"MountainCarContinuous-v0\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action Space: Box(1,)\n",
            "Observation Space: Box(2,)\n",
            "Max Episode Steps: 999\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: 90.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBrlG1t6ceIa"
      },
      "source": [
        "Note: ignore the warning above, it is a relativly inconsequential bug in OpenAI Gym.\n",
        "\n",
        "Atari games, like breakout can use an observation space that is either equal to the size of the Atari screen (210x160) or even use the RAM memory of the Atari (128 bytes) to determine the state of the game.  Yes thats bytes, not kilobytes!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "ndTb-9pgJizW",
        "outputId": "8d1bf60a-ace3-4a6c-ac5b-c2f0c70cc319"
      },
      "source": [
        "query_environment(\"Breakout-v0\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action Space: Discrete(4)\n",
            "Observation Space: Box(210, 160, 3)\n",
            "Max Episode Steps: 10000\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "Ni1rxzmLKAdH",
        "outputId": "4066d9e0-882b-49b4-d399-e3a82c3a2a84"
      },
      "source": [
        "query_environment(\"Breakout-ram-v0\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action Space: Discrete(4)\n",
            "Observation Space: Box(128,)\n",
            "Max Episode Steps: 10000\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E253PBGPRuw"
      },
      "source": [
        "### Render OpenAI Gym Environments from CoLab\n",
        "\n",
        "It is possible to visualize the game your agent is playing, even on CoLab.  This section provides information on how to generate a video in CoLab that shows you an episode of the game your agent is playing. This video process is based on suggestions found [here](https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t).\n",
        "\n",
        "Begin by installing **pyvirtualdisplay** and **python-opengl**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF92FCzZMWPn"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS7L8kFMLkjN"
      },
      "source": [
        "Next, we install needed requirements to display an Atari game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78BfQoQKOq7z",
        "outputId": "90ca9b32-279a-49d8-e7f8-be9819651371"
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjTHm2SpLz10"
      },
      "source": [
        "Next we define functions used to show the video by adding it to the CoLab notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9RpF49oOsZj"
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment \n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6NATj-kNADT"
      },
      "source": [
        "Now we are ready to play the game.  We use a simple random agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "XDKGJ9A3O8fT",
        "outputId": "fd00fc75-4054-49f1-fe87-2fec6c05b640"
      },
      "source": [
        "#env = wrap_env(gym.make(\"MountainCar-v0\"))\n",
        "env = wrap_env(gym.make(\"CartPole-v1\"))\n",
        "#env = wrap_env(gym.make(\"Atlantis-v0\"))\n",
        "\n",
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    action = env.action_space.sample() \n",
        "         \n",
        "    observation, reward, done, info = env.step(action) \n",
        "   \n",
        "        \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}